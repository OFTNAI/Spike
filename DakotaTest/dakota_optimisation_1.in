# DAKOTA INPUT FILE: dakota_pstudy.in for parallel Case 1 (Massively Serial)

environment,
	graphics
	tabular_data

method,                                         
#	conmin_frcg				
	efficient_global
	seed = 123456
	max_iterations = 10000
#	max_function_evaluations = 500

model
	single


variables,					
	continuous_design = 5			
	cdv_initial_point   1.0      1.0      1.0     1.0     1.0
	cdv_lower_bounds    0.50      0.10      0.10      0.10      0.10
	cdv_upper_bounds    1.50      10.0      10.0      10.0      10.0 
	cdv_descriptor      'gmax'      'tau_syn_const'      'conductanceConst_I2E_0'      'conductanceConst_I2E_1'      'conductanceConst_I2E_2'


# Case 1 (Massively Serial): Run Dakota in parallel and launch M-1 serial
#         analysis jobs at once.  Do not specify any evaluation concurrency
#         (handled by parallel scheduler)
#         fork interface is recommended
interface,
	fork
# In an M processor allocation, by default Dakota will configure with
# a master scheduler with M-1 slave analysis processes.  Overriding
# this with evaluation_scheduling peer static will avoid this dedicated 
# master and use all M processors, but then each batch of M analyses will 
# have to complete before the next M are scheduled.  This may be useful if 
# all evaluations are known to take the same processor time:

#	  evaluation_scheduling peer static

# Dynamic scheduling may also be specified. In this mode, the first peer will
# attempt to act as both a master scheduler and an evaluation server. 
# Consequently, all M processors will be used as in the static case. Unlike the
# static case, evaluations are delegated to servers on the fly, making dynamic
# scheduling the better choice when evaluations may be of varying duration.

#         evaluation_scheduling peer dynamic

	  analysis_driver = 'single_simulation_driver'
	    parameters_file = 'params.in'
	    results_file = 'results.out'
	    file_tag
	    file_save

responses,
	objective_functions = 1
	sense = 'max'
	#nonlinear_inequality_constraints = 2
	no_gradients
	no_hessians
